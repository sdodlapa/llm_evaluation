"""
Lightweight model loader for small/medium models

Handles efficient loading and management of models â‰¤30B parameters
with optimizations for single-GPU execution.
"""

import logging
import time
from typing import Dict, Any, Optional, Tuple
import torch
from pathlib import Path

# Import existing vLLM components
try:
    from vllm import LLM, SamplingParams
    from vllm.model_executor.model_loader import get_model
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False
    logging.warning("vLLM not available, using fallback implementation")

from core_shared.model_registry.enhanced_model_config import EnhancedModelConfig

# Import AOT compiler (with fallback if not available)
try:
    from engines.shared.aot_compiler import AOTModelCompiler, is_aot_available
    AOT_COMPILER_AVAILABLE = True
except ImportError:
    AOT_COMPILER_AVAILABLE = False
    logging.warning("AOT compiler not available - using standard loading only")


logger = logging.getLogger(__name__)


class LightweightModelLoader:
    """Optimized model loader for lightweight engine"""
    
    def __init__(self):
        self._cache_dir = Path("./model_cache")
        self._cache_dir.mkdir(exist_ok=True)
        self._initialized = False
        
        # Initialize AOT compiler if available
        self.aot_compiler = None
        self._aot_enabled = False
        
        if AOT_COMPILER_AVAILABLE and is_aot_available():
            try:
                self.aot_compiler = AOTModelCompiler(
                    cache_dir=str(self._cache_dir / "compiled"),
                    enable_aot=True
                )
                self._aot_enabled = True
                logger.info("AOT compilation enabled for lightweight model loader")
            except Exception as e:
                logger.warning(f"Failed to initialize AOT compiler: {e}")
                self._aot_enabled = False
        else:
            logger.info("AOT compilation not available - using standard loading")
    
    def initialize(self) -> bool:
        """Initialize the model loader"""
        try:
            # Check GPU availability (allow CPU-only for testing)
            if not torch.cuda.is_available():
                logger.warning("CUDA not available, using CPU-only mode for testing")
            
            # Check vLLM availability
            if not VLLM_AVAILABLE:
                logger.warning("vLLM not available, using basic fallback")
            
            self._initialized = True
            logger.info("Lightweight model loader initialized")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize model loader: {e}")
            return False
    
    def load_model(self, model_config: EnhancedModelConfig) -> Dict[str, Any]:
        """Load model with optimizations for lightweight engine
        
        Args:
            model_config: Enhanced model configuration
            
        Returns:
            Dict containing model, tokenizer, and metadata
        """
        if not self._initialized:
            raise RuntimeError("Model loader not initialized")
        
        logger.info(f"Loading model {model_config.model_name}")
        load_start = time.time()
        
        try:
            # Check if AOT compilation is available and supported for this model
            if (self._aot_enabled and 
                self.aot_compiler and 
                self.aot_compiler.is_model_supported(model_config)):
                
                logger.info(f"Attempting AOT compilation for {model_config.model_name}")
                return self._load_with_aot_compilation(model_config)
            else:
                # Fall back to standard loading
                if VLLM_AVAILABLE:
                    return self._load_with_vllm(model_config)
                else:
                    return self._load_with_fallback(model_config)
                
        except Exception as e:
            logger.error(f"Failed to load model {model_config.model_name}: {e}")
            raise
    
    def _load_with_vllm(self, model_config: EnhancedModelConfig) -> Dict[str, Any]:
        """Load model using vLLM backend"""
        # Get vLLM configuration
        vllm_config = model_config.get_vllm_config()
        
        # Apply lightweight optimizations
        if hasattr(model_config, 'lightweight_optimizations'):
            opts = model_config.lightweight_optimizations
            if opts.fast_loading:
                vllm_config["load_format"] = "auto"
            if opts.memory_mapping:
                vllm_config["enable_lora"] = False  # Disable LoRA for simpler loading
        
        logger.info(f"Loading {model_config.model_name} with vLLM config: {vllm_config}")
        
        # Create LLM instance
        llm = LLM(**vllm_config)
        
        # Create sampling parameters (can be adjusted per request)
        sampling_params = SamplingParams(
            temperature=model_config.agent_temperature,
            max_tokens=512,  # Default, can be overridden
            top_p=0.9
        )
        
        return {
            "model": llm,
            "tokenizer": llm.get_tokenizer(),
            "sampling_params": sampling_params,
            "backend": "vllm",
            "config": vllm_config,
            "model_name": model_config.model_name,
            "size_gb": model_config.size_gb
        }
    
    def _load_with_fallback(self, model_config: EnhancedModelConfig) -> Dict[str, Any]:
        """Fallback implementation without vLLM"""
        logger.warning(f"Loading {model_config.model_name} with fallback implementation")
        
        # This would implement a basic transformers loading approach
        # For now, return a mock implementation for testing
        return {
            "model": MockModel(model_config.model_name),
            "tokenizer": MockTokenizer(),
            "backend": "fallback",
            "config": {},
            "model_name": model_config.model_name,
            "size_gb": model_config.size_gb
        }
    
    def validate_model_compatibility(self, model_config: EnhancedModelConfig) -> bool:
        """Check if model is compatible with lightweight loader"""
        # Size check
        if model_config.size_gb > 60.0:
            logger.warning(f"Model {model_config.model_name} too large for lightweight loader")
            return False
        
        # Multi-GPU check
        if model_config.tensor_parallel_size > 1 or model_config.pipeline_parallel_size > 1:
            logger.warning(f"Model {model_config.model_name} requires multi-GPU, not suitable for lightweight loader")
            return False
        
        return True
    
    def get_memory_requirements(self, model_config: EnhancedModelConfig) -> Dict[str, float]:
        """Estimate memory requirements for model loading"""
        base_memory = model_config.size_gb
        
        # Add overhead for activation memory and evaluation
        total_memory = base_memory * 2.0  # Model + activations
        total_memory += 2.0  # Evaluation overhead
        
        # Apply optimization adjustments
        if hasattr(model_config, 'lightweight_optimizations'):
            opts = model_config.lightweight_optimizations
            if opts.memory_mapping:
                total_memory *= 0.9
            if opts.use_flash_attention:
                total_memory *= 0.85
        
        return {
            "model_memory_gb": base_memory,
            "total_gpu_memory_gb": total_memory,
            "system_memory_gb": max(8.0, base_memory * 0.5)
        }
    
    def _load_with_aot_compilation(self, model_config: EnhancedModelConfig) -> Dict[str, Any]:
        """Load model with AOT compilation optimization"""
        logger.info(f"Loading {model_config.model_name} with AOT compilation")
        
        # Generate cache key for this configuration
        example_inputs = self._generate_example_inputs(model_config)
        cache_key = self.aot_compiler._generate_cache_key(model_config, example_inputs)
        
        # Try to load from cache first
        compiled_model = self.aot_compiler.load_compiled_model(cache_key)
        
        if compiled_model is not None:
            # Use cached compiled model
            logger.info(f"Using cached AOT compiled model for {model_config.model_name}")
            return self._create_model_info_from_compiled(
                compiled_model, model_config, cache_key
            )
        else:
            # Load and compile model for first time
            logger.info(f"Compiling {model_config.model_name} with AOT for first time")
            return self._load_and_compile_model(model_config, example_inputs, cache_key)
    
    def _generate_example_inputs(self, model_config: EnhancedModelConfig) -> Tuple:
        """Generate representative input tensors for AOT compilation"""
        try:
            # Create sample input tensors based on model configuration
            batch_size = 1
            seq_length = 512  # Representative sequence length
            
            # Generate input_ids tensor (most common input for LLMs)
            input_ids = torch.randint(
                low=1, high=32000,  # Typical vocabulary size range
                size=(batch_size, seq_length), 
                dtype=torch.long
            )
            
            # Move to GPU if available
            if torch.cuda.is_available():
                input_ids = input_ids.cuda()
            
            return (input_ids,)
            
        except Exception as e:
            logger.error(f"Failed to generate example inputs: {e}")
            # Fallback to CPU tensors
            input_ids = torch.randint(1, 1000, (1, 512), dtype=torch.long)
            return (input_ids,)
    
    def _load_and_compile_model(self, 
                               model_config: EnhancedModelConfig, 
                               example_inputs: Tuple, 
                               cache_key: str) -> Dict[str, Any]:
        """Load model and perform AOT compilation"""
        
        try:
            # Step 1: Load model normally first
            logger.debug("Loading model with standard method for AOT compilation")
            standard_model_info = self._load_with_vllm(model_config) if VLLM_AVAILABLE else self._load_with_fallback(model_config)
            
            # Step 2: Extract the underlying PyTorch model for compilation
            original_model = standard_model_info["model"]
            
            # For vLLM models, we need to extract the actual PyTorch model
            if hasattr(original_model, 'llm_engine'):
                # vLLM case - extract PyTorch model from engine
                try:
                    pytorch_model = original_model.llm_engine.model_executor.driver_worker.model_runner.model
                    logger.debug("Extracted PyTorch model from vLLM engine")
                except AttributeError:
                    logger.warning("Could not extract PyTorch model from vLLM - using mock compilation")
                    pytorch_model = None
            else:
                # Direct PyTorch model
                pytorch_model = original_model
            
            # Step 3: Compile the model (if we have a PyTorch model)
            compiled_model = None
            if pytorch_model is not None:
                logger.info("Starting AOT compilation...")
                compiled_model = self.aot_compiler.compile_model_aot(
                    pytorch_model, 
                    example_inputs, 
                    model_config, 
                    compilation_mode="default"
                )
            
            if compiled_model is None:
                # Compilation failed or not possible, use standard model
                logger.warning(f"AOT compilation failed for {model_config.model_name}, using standard loading")
                standard_model_info["aot_compiled"] = False
                standard_model_info["aot_compilation_attempted"] = True
                return standard_model_info
            
            # Step 4: Save compiled model for future use
            if self.aot_compiler.save_compiled_model(cache_key):
                logger.info(f"Saved AOT compiled model for {model_config.model_name}")
            
            # Step 5: Create optimized model info
            return self._create_model_info_from_compiled(
                compiled_model, model_config, cache_key, base_info=standard_model_info
            )
            
        except Exception as e:
            logger.error(f"AOT compilation pipeline failed: {e}")
            logger.debug("Compilation error details:", exc_info=True)
            
            # Fallback to standard loading
            logger.info("Falling back to standard model loading")
            fallback_info = self._load_with_vllm(model_config) if VLLM_AVAILABLE else self._load_with_fallback(model_config)
            fallback_info["aot_compiled"] = False
            fallback_info["aot_compilation_attempted"] = True
            fallback_info["aot_compilation_error"] = str(e)
            return fallback_info
    
    def _create_model_info_from_compiled(self, 
                                       compiled_model: Any, 
                                       model_config: EnhancedModelConfig, 
                                       cache_key: str,
                                       base_info: Optional[Dict] = None) -> Dict[str, Any]:
        """Create model info dict for AOT compiled model"""
        
        if base_info is None:
            # Create minimal base info for cached case
            base_info = {
                "model": None,  # Will be replaced with compiled model
                "tokenizer": None,  # Need to load separately for cached models
                "backend": "aot_compiled",
                "config": {},
                "model_name": model_config.model_name,
                "size_gb": getattr(model_config, 'size_gb', 0)
            }
            
            # For cached models, we need to load a tokenizer separately
            try:
                if VLLM_AVAILABLE:
                    # Create minimal vLLM instance just for tokenizer
                    temp_vllm_config = model_config.get_vllm_config()
                    temp_llm = LLM(**temp_vllm_config)
                    base_info["tokenizer"] = temp_llm.get_tokenizer()
                    # Clean up the temporary LLM
                    del temp_llm
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None
                else:
                    base_info["tokenizer"] = MockTokenizer()
            except Exception as e:
                logger.warning(f"Could not load tokenizer for cached model: {e}")
                base_info["tokenizer"] = MockTokenizer()
        
        # Replace model with compiled version and add AOT metadata
        base_info.update({
            "model": compiled_model,
            "backend": "aot_compiled",
            "aot_compiled": True,
            "aot_cache_key": cache_key,
            "compilation_stats": self.aot_compiler.compilation_stats.get(cache_key, {})
        })
        
        # Add load time for AOT cached models (very fast)
        if "load_time" not in base_info:
            base_info["load_time"] = 2.0  # AOT cached models load very quickly
        
        logger.info(f"Created AOT compiled model info for {model_config.model_name}")
        return base_info


class MockModel:
    """Mock model for testing without actual model loading"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
    
    def generate(self, prompts, sampling_params=None):
        """Mock generation for testing"""
        return [MockOutput(f"Generated response for: {prompt[:50]}...") for prompt in prompts]


class MockTokenizer:
    """Mock tokenizer for testing"""
    
    def encode(self, text: str):
        return list(range(len(text.split())))
    
    def decode(self, tokens):
        return " ".join([f"token_{i}" for i in tokens])


class MockOutput:
    """Mock output for testing"""
    
    def __init__(self, text: str):
        self.outputs = [MockCompletionOutput(text)]


class MockCompletionOutput:
    """Mock completion output for testing"""
    
    def __init__(self, text: str):
        self.text = text