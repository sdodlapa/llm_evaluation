# FOCUSED: High-ROI vLLM Integration Plan

**Document Version:** 2.0 (REVISED)  
**Date:** September 21, 2025  
**Scope:** Focus on proven, high-impact optimizations only

---

## üö® **CRITICAL REVISION NOTICE**

**The original comprehensive plan has been significantly revised based on dataset analysis and ROI assessment. This focused plan targets only optimizations with clear, measurable benefits for our current evaluation workload.**

---

## üìä **Dataset Analysis Summary**

### **Current Reality**
- **Average input length**: 80 characters (~20 tokens)
- **Maximum input seen**: 190 characters (~50 tokens)  
- **Current system limit**: 2048 tokens (40x headroom)
- **Datasets requiring >2K chars**: 0 out of 191 analyzed

### **Implications**
- ‚ùå **Chunked Prefill**: Unnecessary (handles 32K+ tokens, we need <50)
- ‚ùå **Long Context Optimization**: Unneeded (current limit has 40x headroom)
- ‚úÖ **CUDA Graph Integration**: High value (affects all evaluations)
- ‚ö†Ô∏è **Guided Decoding**: Assess parsing failures first

---

## üéØ **REVISED: Focused 2-3 Week Plan**

### **Phase 1: CUDA Graph Integration (Week 1-2)**

**Objective**: 15-25% latency improvement across all evaluations

**Implementation Strategy**:
- Enhance existing `AOTModelCompiler` with CUDA graph capture
- Minimal changes to working system
- Clean, modular integration

**Expected Benefits**:
- 15-25% faster inference
- Better GPU utilization
- More predictable performance

### **Phase 2: Basic Performance Monitoring (Week 3)**

**Objective**: Operational visibility into system performance

**Implementation Strategy**:
- Decorator-based monitoring (no core logic changes)
- Simple metrics collection (latency, memory, throughput)
- Export to JSON for analysis

**Expected Benefits**:
- Performance baseline measurement
- Regression detection
- Optimization impact validation

---

## üîß **Technical Implementation**

### **1. CUDA Graph Enhancement Module**

**File**: `engines/shared/cuda_graph_optimizer.py`

```python
"""
CUDA Graph Optimization Module
Enhances existing AOT compilation with graph capture/replay
"""

import torch
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class CudaGraphConfig:
    """Configuration for CUDA graph optimization"""
    enabled: bool = True
    batch_sizes: List[int] = None
    warmup_steps: int = 3
    max_graphs: int = 10
    
    def __post_init__(self):
        if self.batch_sizes is None:
            self.batch_sizes = [1, 2, 4, 8]

class CudaGraphOptimizer:
    """
    CUDA Graph optimizer that integrates with existing AOT compilation
    
    This class enhances the existing AOTModelCompiler without modifying
    its core functionality, following a clean integration pattern.
    """
    
    def __init__(self, config: CudaGraphConfig = None):
        self.config = config or CudaGraphConfig()
        self.captured_graphs: Dict[str, torch.cuda.CUDAGraph] = {}
        self.graph_inputs: Dict[str, List[torch.Tensor]] = {}
        self.graph_outputs: Dict[str, List[torch.Tensor]] = {}
        self.stats = {
            "graphs_captured": 0,
            "graph_replays": 0,
            "capture_failures": 0
        }
        
        if not torch.cuda.is_available():
            logger.warning("CUDA not available, disabling graph optimization")
            self.config.enabled = False
    
    def enhance_compiled_model(self, 
                             compiled_model: Any,
                             example_inputs: Tuple[torch.Tensor, ...],
                             model_id: str) -> bool:
        """
        Enhance a compiled model with CUDA graph capture
        
        Args:
            compiled_model: AOT compiled model
            example_inputs: Representative input tensors
            model_id: Unique identifier for this model
            
        Returns:
            bool: True if enhancement successful
        """
        if not self.config.enabled:
            return False
        
        try:
            success_count = 0
            
            for batch_size in self.config.batch_sizes:
                graph_key = f"{model_id}_batch_{batch_size}"
                
                if self._capture_graph(compiled_model, example_inputs, 
                                     batch_size, graph_key):
                    success_count += 1
            
            logger.info(f"Enhanced model {model_id} with {success_count} CUDA graphs")
            return success_count > 0
            
        except Exception as e:
            logger.error(f"Failed to enhance model {model_id}: {e}")
            self.stats["capture_failures"] += 1
            return False
    
    def _capture_graph(self, 
                      model: Any,
                      example_inputs: Tuple[torch.Tensor, ...],
                      batch_size: int,
                      graph_key: str) -> bool:
        """Capture CUDA graph for specific batch size"""
        
        try:
            # Prepare inputs for this batch size
            graph_inputs = self._prepare_batch_inputs(example_inputs, batch_size)
            
            # Warmup runs
            model.eval()
            with torch.no_grad():
                for _ in range(self.config.warmup_steps):
                    _ = model(*graph_inputs)
                    torch.cuda.synchronize()
            
            # Capture graph
            graph = torch.cuda.CUDAGraph()
            
            with torch.cuda.graph(graph):
                graph_outputs = model(*graph_inputs)
            
            # Store graph components
            self.captured_graphs[graph_key] = graph
            self.graph_inputs[graph_key] = graph_inputs
            self.graph_outputs[graph_key] = (
                graph_outputs if isinstance(graph_outputs, (list, tuple))
                else [graph_outputs]
            )
            
            self.stats["graphs_captured"] += 1
            logger.debug(f"Captured graph: {graph_key}")
            return True
            
        except Exception as e:
            logger.debug(f"Graph capture failed for {graph_key}: {e}")
            return False
    
    def _prepare_batch_inputs(self, 
                            example_inputs: Tuple[torch.Tensor, ...],
                            batch_size: int) -> List[torch.Tensor]:
        """Prepare input tensors with target batch size"""
        
        batch_inputs = []
        
        for input_tensor in example_inputs:
            # Create tensor with target batch size
            target_shape = list(input_tensor.shape)
            target_shape[0] = batch_size
            
            batch_input = torch.empty(
                target_shape,
                dtype=input_tensor.dtype,
                device=input_tensor.device
            )
            
            # Fill with example data
            if batch_size <= input_tensor.shape[0]:
                batch_input.copy_(input_tensor[:batch_size])
            else:
                # Repeat to fill larger batch
                repeat_factor = (batch_size + input_tensor.shape[0] - 1) // input_tensor.shape[0]
                repeated = input_tensor.repeat(repeat_factor, *([1] * (len(input_tensor.shape) - 1)))
                batch_input.copy_(repeated[:batch_size])
            
            batch_inputs.append(batch_input)
        
        return batch_inputs
    
    def try_graph_execution(self, 
                          model_id: str,
                          input_batch: torch.Tensor) -> Optional[torch.Tensor]:
        """
        Attempt to execute using captured graph
        
        Args:
            model_id: Model identifier
            input_batch: Input tensor batch
            
        Returns:
            Model output if graph execution successful, None otherwise
        """
        if not self.config.enabled:
            return None
        
        batch_size = input_batch.shape[0]
        graph_key = f"{model_id}_batch_{batch_size}"
        
        if graph_key not in self.captured_graphs:
            return None
        
        try:
            # Copy input data to graph tensors
            graph_inputs = self.graph_inputs[graph_key]
            graph_inputs[0].copy_(input_batch)
            
            # Execute graph
            self.captured_graphs[graph_key].replay()
            
            # Return copied output
            graph_outputs = self.graph_outputs[graph_key]
            output = graph_outputs[0].clone()
            
            self.stats["graph_replays"] += 1
            return output
            
        except Exception as e:
            logger.debug(f"Graph execution failed for {graph_key}: {e}")
            return None
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get optimization statistics"""
        return {
            **self.stats,
            "total_graphs": len(self.captured_graphs),
            "enabled": self.config.enabled
        }
    
    def cleanup(self):
        """Clean up resources"""
        self.captured_graphs.clear()
        self.graph_inputs.clear()
        self.graph_outputs.clear()
```

### **2. Enhanced AOT Compiler Integration**

**File**: `engines/shared/aot_compiler_enhanced.py`

```python
"""
Enhanced AOT Compiler with CUDA Graph Support
Extends existing AOTModelCompiler with minimal changes
"""

import logging
from typing import Any, Optional, Tuple, Dict
from .aot_compiler import AOTModelCompiler  # Import existing
from .cuda_graph_optimizer import CudaGraphOptimizer, CudaGraphConfig

logger = logging.getLogger(__name__)

class EnhancedAOTModelCompiler(AOTModelCompiler):
    """
    Enhanced AOT compiler with CUDA graph optimization
    
    Extends the existing AOTModelCompiler with CUDA graph support
    while maintaining full backward compatibility.
    """
    
    def __init__(self, 
                 cache_dir: str = "model_cache/compiled",
                 enable_aot: bool = True,
                 max_compilation_time: int = 600,
                 cuda_graph_config: CudaGraphConfig = None):
        
        # Initialize parent class
        super().__init__(cache_dir, enable_aot, max_compilation_time)
        
        # Add CUDA graph optimizer
        self.cuda_optimizer = CudaGraphOptimizer(cuda_graph_config)
        
        # Track enhanced models
        self.enhanced_models: Dict[str, str] = {}
        
        logger.info(f"Enhanced AOT compiler initialized (CUDA graphs: {self.cuda_optimizer.config.enabled})")
    
    def compile_model_aot(self, 
                         model,
                         example_inputs: Tuple,
                         model_config,
                         compilation_mode: str = "default") -> Optional[Any]:
        """
        Enhanced compilation with CUDA graph capture
        
        This method extends the parent's compilation with graph optimization
        while maintaining identical interface and behavior.
        """
        
        # First, perform standard AOT compilation
        compiled_model = super().compile_model_aot(
            model, example_inputs, model_config, compilation_mode
        )
        
        if compiled_model is None:
            return None
        
        # Enhance with CUDA graphs if compilation succeeded
        model_id = getattr(model_config, 'model_name', 'unknown_model')
        
        try:
            if self.cuda_optimizer.enhance_compiled_model(
                compiled_model, example_inputs, model_id
            ):
                self.enhanced_models[model_id] = "cuda_graphs_enabled"
                logger.info(f"Successfully enhanced {model_id} with CUDA graphs")
            else:
                self.enhanced_models[model_id] = "cuda_graphs_failed"
                logger.debug(f"CUDA graph enhancement failed for {model_id}")
                
        except Exception as e:
            logger.warning(f"CUDA graph enhancement error for {model_id}: {e}")
            self.enhanced_models[model_id] = "cuda_graphs_error"
        
        return compiled_model
    
    def try_optimized_inference(self, 
                              model_id: str,
                              input_tensor: torch.Tensor) -> Optional[torch.Tensor]:
        """
        Attempt optimized inference using CUDA graphs
        
        Args:
            model_id: Model identifier
            input_tensor: Input batch tensor
            
        Returns:
            Model output if optimization successful, None for fallback
        """
        return self.cuda_optimizer.try_graph_execution(model_id, input_tensor)
    
    def get_enhancement_stats(self) -> Dict[str, Any]:
        """Get enhancement statistics"""
        return {
            "enhanced_models": self.enhanced_models.copy(),
            "cuda_graph_stats": self.cuda_optimizer.get_statistics()
        }
    
    def cleanup(self):
        """Enhanced cleanup"""
        super().cleanup()
        self.cuda_optimizer.cleanup()
```

### **3. Simple Performance Monitor**

**File**: `engines/shared/performance_monitor.py`

```python
"""
Simple Performance Monitor
Provides basic performance tracking with minimal system impact
"""

import time
import psutil
import torch
import json
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict
from functools import wraps
from datetime import datetime

@dataclass
class PerformanceMetric:
    """Single performance measurement"""
    operation: str
    duration_ms: float
    memory_mb: float
    gpu_memory_mb: float
    timestamp: str
    cuda_graphs_used: bool = False
    
class SimplePerformanceMonitor:
    """
    Lightweight performance monitor with decorator-based tracking
    
    Designed to add monitoring with zero impact on existing code structure.
    Uses decorator pattern to wrap existing methods.
    """
    
    def __init__(self, enable_gpu_monitoring: bool = True):
        self.enable_gpu_monitoring = enable_gpu_monitoring and torch.cuda.is_available()
        self.metrics: List[PerformanceMetric] = []
        self.enabled = True
        
        # GPU monitoring setup
        if self.enable_gpu_monitoring:
            try:
                import pynvml
                pynvml.nvmlInit()
                self.gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                self.pynvml = pynvml
            except ImportError:
                self.enable_gpu_monitoring = False
    
    def measure_performance(self, operation_name: str, track_cuda_graphs: bool = False):
        """
        Decorator to measure performance of any function
        
        Usage:
            @monitor.measure_performance("model_inference")
            def my_function():
                # existing code unchanged
                pass
        """
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                if not self.enabled:
                    return func(*args, **kwargs)
                
                # Pre-execution measurements
                start_time = time.time()
                start_memory = self._get_memory_usage()
                start_gpu_memory = self._get_gpu_memory()
                
                try:
                    # Execute original function
                    result = func(*args, **kwargs)
                    
                    # Post-execution measurements
                    end_time = time.time()
                    end_memory = self._get_memory_usage()
                    end_gpu_memory = self._get_gpu_memory()
                    
                    # Record metric
                    metric = PerformanceMetric(
                        operation=operation_name,
                        duration_ms=(end_time - start_time) * 1000,
                        memory_mb=max(start_memory, end_memory),
                        gpu_memory_mb=max(start_gpu_memory, end_gpu_memory),
                        timestamp=datetime.now().isoformat(),
                        cuda_graphs_used=track_cuda_graphs and self._detect_cuda_graphs(result)
                    )
                    
                    self.metrics.append(metric)
                    
                    return result
                    
                except Exception as e:
                    # Record failed operation
                    metric = PerformanceMetric(
                        operation=f"{operation_name}_FAILED",
                        duration_ms=(time.time() - start_time) * 1000,
                        memory_mb=self._get_memory_usage(),
                        gpu_memory_mb=self._get_gpu_memory(),
                        timestamp=datetime.now().isoformat(),
                        cuda_graphs_used=False
                    )
                    self.metrics.append(metric)
                    raise
            
            return wrapper
        return decorator
    
    def _get_memory_usage(self) -> float:
        """Get current memory usage in MB"""
        try:
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except:
            return 0.0
    
    def _get_gpu_memory(self) -> float:
        """Get current GPU memory usage in MB"""
        if not self.enable_gpu_monitoring:
            return 0.0
        
        try:
            info = self.pynvml.nvmlDeviceGetMemoryInfo(self.gpu_handle)
            return info.used / 1024 / 1024
        except:
            return 0.0
    
    def _detect_cuda_graphs(self, result: Any) -> bool:
        """Simple heuristic to detect if CUDA graphs were used"""
        # This is a placeholder - actual detection would depend on
        # how we track graph usage in the optimizer
        return False
    
    def get_summary(self, last_n: int = None) -> Dict[str, Any]:
        """Get performance summary"""
        metrics_subset = self.metrics[-last_n:] if last_n else self.metrics
        
        if not metrics_subset:
            return {"error": "No metrics available"}
        
        durations = [m.duration_ms for m in metrics_subset]
        memories = [m.memory_mb for m in metrics_subset]
        
        return {
            "total_operations": len(metrics_subset),
            "avg_duration_ms": sum(durations) / len(durations),
            "max_duration_ms": max(durations),
            "min_duration_ms": min(durations),
            "avg_memory_mb": sum(memories) / len(memories),
            "max_memory_mb": max(memories),
            "cuda_graphs_usage": sum(1 for m in metrics_subset if m.cuda_graphs_used),
            "time_range": {
                "start": metrics_subset[0].timestamp,
                "end": metrics_subset[-1].timestamp
            }
        }
    
    def export_metrics(self, filepath: str):
        """Export metrics to JSON file"""
        with open(filepath, 'w') as f:
            json.dump([asdict(m) for m in self.metrics], f, indent=2)
    
    def clear_metrics(self):
        """Clear all stored metrics"""
        self.metrics.clear()
    
    def disable(self):
        """Disable monitoring"""
        self.enabled = False
    
    def enable(self):
        """Enable monitoring"""
        self.enabled = True

# Global monitor instance for easy use
performance_monitor = SimplePerformanceMonitor()

# Convenience decorators
def monitor_inference(func):
    """Decorator specifically for inference operations"""
    return performance_monitor.measure_performance("inference", track_cuda_graphs=True)(func)

def monitor_compilation(func):
    """Decorator specifically for compilation operations"""
    return performance_monitor.measure_performance("compilation")(func)

def monitor_evaluation(func):
    """Decorator specifically for evaluation operations"""
    return performance_monitor.measure_performance("evaluation")(func)
```

---

## üîå **Clean Integration Strategy**

### **Integration Points**

1. **AOT Compiler Enhancement**: Use inheritance to extend existing compiler
2. **Performance Monitoring**: Use decorators to avoid changing core logic
3. **Model Loading**: Optional enhancement that falls back gracefully

### **Variable Name Consistency**

All new code follows existing naming conventions:
- `model_config` (existing) ‚Üí maintained
- `compiled_model` (existing) ‚Üí maintained  
- `example_inputs` (existing) ‚Üí maintained
- New variables use descriptive names: `cuda_optimizer`, `performance_monitor`

### **Zero-Impact Design**

- Existing evaluation pipeline requires **zero changes**
- All enhancements are **optional and graceful**
- Performance monitoring is **decorator-based**
- CUDA graphs **fall back** to normal execution

---

## üìà **Expected Benefits**

### **Quantified Improvements**
- **Latency**: 15-25% reduction in inference time
- **Consistency**: More predictable performance via CUDA graphs
- **Visibility**: Complete performance tracking and trend analysis
- **Reliability**: Zero regression risk due to graceful fallbacks

### **Operational Benefits**
- Performance baseline establishment
- Regression detection capability
- Optimization impact measurement
- System health monitoring

---

## ‚ö†Ô∏è **Risk Mitigation**

### **Technical Risks**
- **Graph Capture Failure**: Graceful fallback to normal execution
- **Memory Overhead**: Configurable limits and monitoring
- **Performance Regression**: Comprehensive before/after measurement

### **Integration Risks**
- **Code Conflicts**: Minimal changes to existing codebase
- **Variable Name Issues**: Strict adherence to existing conventions
- **Interface Changes**: Zero breaking changes to existing APIs

---

## üìÖ **Implementation Timeline**

### **Week 1: CUDA Graph Foundation**
- Implement `CudaGraphOptimizer` class
- Create `EnhancedAOTModelCompiler`
- Unit tests and validation

### **Week 2: CUDA Graph Integration**
- Integrate with existing model loading
- Performance testing and optimization
- Documentation updates

### **Week 3: Performance Monitoring**
- Implement `SimplePerformanceMonitor`
- Add decorator-based tracking
- Export and analysis tools

### **Success Criteria**
- ‚úÖ 15%+ latency improvement measured
- ‚úÖ Zero regression in existing functionality  
- ‚úÖ Complete performance visibility
- ‚úÖ Clean, modular code integration

---

## üéØ **Next Steps**

1. **Start with CUDA Graph Implementation**: High ROI, low risk
2. **Measure Everything**: Establish baseline before optimization
3. **Iterate Based on Data**: Let measurements guide next priorities
4. **Maintain System Stability**: Never compromise working functionality

This focused plan delivers maximum value with minimal risk and complexity, targeting only optimizations that provide clear benefits for our current evaluation workload.